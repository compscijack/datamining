{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the necessary packages\n",
    "\n",
    "# Packages for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Packages for visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Read the processed survey response data\n",
    "\n",
    "Let's import the processed dataset from the previous session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('./data/class_survey_data_cleaned.csv')\n",
    "# To check the first few enteries of a dataframe\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the columns encode the following information:\n",
    "\n",
    "- TIMESTAMP = 'Timestamp'\n",
    "- BACKGROUND_INDUSTRY = 'What main industry have you worked in?'\n",
    "- BACKGROUND_YEARS_PROFESSIONAL = 'How many years professional experience do you have?'\n",
    "- BACKGROUND_YEARS_PROGRAMMING = 'How many years programming experience do you have?'\n",
    "- BACKGROUND_SKILLS = 'What key experience do you have?'\n",
    "- IMPORT_DATA_MANAGEMENT = 'Data management'\n",
    "- IMPORT_STATISTICS = 'Statistics'\n",
    "- IMPORT_VISUALISATION = 'Visualisation'\n",
    "- IMPORT_MACHINE_LEARNING = 'Machine Learning & Data Mining'\n",
    "- IMPORT_SOFTWARE_ENGINEERING = 'Software Engineering'\n",
    "- IMPORT_COMMUNICATION = 'Communication'\n",
    "- GOALS_DEFINITION = 'How would you define Data Science in one sentence?'\n",
    "- GOALS_SKILLS = 'What key skills do you want to learn?'\n",
    "- GOALS_ROLE = 'What kind of role would you like to go into?'\n",
    "- GOALS_INDUSTRY = 'What industry would you like to go into?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Visualisation with Seaborn\n",
    "\n",
    "### 1) Making a histogram\n",
    "\n",
    "`seaborn` provides functionality for creating various plots. Let's start with a `histogram` to visualise the distribution of years of professional experience or programming among the respondents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create histogram for programming experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram to show the distribution of BACKGROUND_YEARS_PROFESSIONAL\n",
    "\n",
    "# Setting the style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(7, 4)) # Setting the size of the figure\n",
    "sns.histplot(data = data, x = 'BACKGROUND_YEARS_PROFESSIONAL', binwidth = 5)\n",
    "plt.title('Distribution of years of professional experience')\n",
    "plt.xlabel('Years of professional experience')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows the distribution of the years of professional experience among the survey respondents. As can be seen, a majority of respondents have between 0 and years of professional experience, indicating a younger or more early-career demographic across the respondents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*\n",
    "### TODO: Create histogram for programming experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Making a bar chart\n",
    "\n",
    "Here we will display the distribution of the number of responses for each of the six importance variables. For this, we will create a function that takes as input the survey dataset and the column name (one of the six importance variables), and then creates a bar plot showing the count of each rating value for that particular aspect. The function will be used to create six bar plots for the importantce variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_importance_plot(data, column):\n",
    "    \"\"\"\n",
    "    This function displays a bar plot for the distribution of number of responses\n",
    "    for a given importance rating variable in the survey data.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The survey data.\n",
    "    column (str): The column name for the importance rating variable.\n",
    "    \"\"\"\n",
    "\n",
    "    sns.countplot(data = data, x = column, color = 'grey')\n",
    "    plt.ylim(0, 70)\n",
    "    plt.title(column.replace('IMPORT_', '').replace('_', ' ').lower()) # Remove the prefix and underscores from the column name\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Number of responses')\n",
    "    plt.grid(axis = 'both', alpha = 0.5) # Add grid lines to the plot in both directions\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_AREAS = [\n",
    "    'IMPORT_DATA_MANAGEMENT',\n",
    "    'IMPORT_STATISTICS',\n",
    "    'IMPORT_VISUALISATION',\n",
    "    'IMPORT_MACHINE_LEARNING',\n",
    "    'IMPORT_SOFTWARE_ENGINEERING',\n",
    "    'IMPORT_COMMUNICATION'\n",
    "]\n",
    "\n",
    "# Creating bar plots for each of the six importance rating variables\n",
    "for col in IMPORT_AREAS:\n",
    "    plt.figure(figsize = (7, 4))\n",
    "    make_importance_plot(data, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis in each plot represents the rating given by the respondents for the importance of the given area. The y-axis represents the number of responses for each rating. The majority of respondents left high ratings (4 or 5) for most areas, except for software engineering which more diverse opinons and picked at a rating of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average importance ratings for different Data Science aspects\n",
    "\n",
    "To be able to better compare respondents' opinions on the different areas of data science, we will make a single bar plot that shows the average importance ratings for the 6 above aspects.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for bar plot\n",
    "import_means = data[IMPORT_AREAS].mean()\n",
    "# Change index names for better readability\n",
    "import_means.index = import_means.index.str.replace('IMPORT_', '').str.replace('_', ' ').str.lower()\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize = (8, 4))\n",
    "sns.barplot(x = import_means.index, y = import_means.values)\n",
    "plt.title('Average importance ratings for different Data Science aspects')\n",
    "plt.ylabel('Average rating')\n",
    "plt.xticks(rotation = 45)  # Rotate labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Make bar charts of known and future industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Making a scatterplot\n",
    "\n",
    "Finally, let's make a scatterplot to show the relationship between professional and programming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the scatterplot for professional vs programming experience\n",
    "#plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=data, x=\"BACKGROUND_YEARS_PROFESSIONAL\", y=\"BACKGROUND_YEARS_PROGRAMMING\")\n",
    "plt.title('Professional vs programming experience')\n",
    "plt.xlabel('Years of professional experience')\n",
    "plt.ylabel('Years of programming experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a positive relationship between professional experience and programming experience, as respondents with more years of professional experience tend to also have more programming experience. Also, most respondents have less than 5 years of experience both professionally and in programming. There are also potentially 3 outliers, most notably the one with 40 years experience in both programming and professional experience (worth checking this respondent's data closely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Visualising distributions with box plots\n",
    "\n",
    "Mean and standard deviation are not informative for skewed data. `boxplot` is is a good visualisation for viewing and comparing distributions. It also shows outliers, e.g., values greater than `Q3+1.5*IQR` or less than `Q1-1.5*IQR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data for side-by-side boxplots\n",
    "experience_data = data[['BACKGROUND_YEARS_PROFESSIONAL', 'BACKGROUND_YEARS_PROGRAMMING']].melt(var_name='EXPERIENCE_TYPE', value_name='YEARS')\n",
    "\n",
    "# Creating side-by-side boxplots\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.boxplot(data=experience_data, x='EXPERIENCE_TYPE', y='YEARS', color='lightgrey')\n",
    "plt.title('Professional vs programming experience')\n",
    "plt.xticks([0, 1], ['Professional', 'Programming'])\n",
    "plt.xlabel('Type of experience')\n",
    "plt.ylabel('Years experience')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the medians suggest that, on average, respondents have more professional experience than programming experience. In addition, the professional experience is more spread out than the programming experience, with a larger interquartile range. Finally, there are outliers in both distributions, with some individuals having exceptionally high years of either professional or programming experience. This is more pronounced in programming experience, suggesting a few respondents have been programming for an unusually long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Calculating correlation between two variables\n",
    "\n",
    "Pearson's r is the covariance of the two variables divided by the product of their standard deviations. Spearman rho is a common nonparametric test that is used instead of Pearson's r when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Pearson correlation between years of professional experience and years of programming experience\n",
    "pearson_corr = data['BACKGROUND_YEARS_PROFESSIONAL'].corr(data['BACKGROUND_YEARS_PROGRAMMING'], method='pearson')\n",
    "Spearman_corr = data['BACKGROUND_YEARS_PROFESSIONAL'].corr(data['BACKGROUND_YEARS_PROGRAMMING'], method='spearman')\n",
    "\n",
    "print('Pearson correlation coefficient: {:.2f}'.format(pearson_corr))\n",
    "print('Spearman correlation coefficient: {:.2f}'.format(Spearman_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Kendall's tau between importance ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty matrix for the correlation values\n",
    "kendall_corr_matrix = np.zeros((len(IMPORT_AREAS), len(IMPORT_AREAS)))\n",
    "\n",
    "# Calculate Kendall's tau for each pair of importance variables\n",
    "for i, col1 in enumerate(IMPORT_AREAS):\n",
    "    for j, col2 in enumerate(IMPORT_AREAS):\n",
    "        tau, _ = kendalltau(data[col1], data[col2])\n",
    "        kendall_corr_matrix[i, j] = tau\n",
    "\n",
    "# Defining more meaningful labels for the variables\n",
    "meaningful_labels = {\n",
    "    \"IMPORT_DATA_MANAGEMENT\": \"Data Management\",\n",
    "    \"IMPORT_STATISTICS\": \"Statistics\",\n",
    "    \"IMPORT_VISUALISATION\": \"Visualisation\",\n",
    "    \"IMPORT_MACHINE_LEARNING\": \"Machine Learning\",\n",
    "    \"IMPORT_SOFTWARE_ENGINEERING\": \"Software Engineering\",\n",
    "    \"IMPORT_COMMUNICATION\": \"Communication\"\n",
    "}\n",
    "\n",
    "# Convert the matrix to a DataFrame for better readability, using meaningful labels\n",
    "kendall_corr_df = pd.DataFrame(kendall_corr_matrix, \n",
    "                               index = meaningful_labels.values(), \n",
    "                               columns = meaningful_labels.values())\n",
    "kendall_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise the correlation matrix as a heatmap as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a heatmap of the Kendall's tau correlation matrix\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(kendall_corr_df, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5) \n",
    "# rotate the x-axis labels 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Heatmap of Kendall\\'s tau correlation matrix for the importance variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All correlations are positive and particularly strong for statistics and machine learning, and statistics and visualisation. This means that people who rate statistics as important also tend to rate the other two aspects as important. It is also worth highlighting that machine learning and communication are weakly correlated (tau = 0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Visualising text data\n",
    "\n",
    "#### Extracting words and counting their frequencies\n",
    "\n",
    "We will break the texts under the \"data science definition\" column into words. During the process, we will also remove stop words, which are words that are commonly used in English but do not add a strong meaning, such as \"the\", \"a\", \"an\", \"in\", \"at\", \"him\", \"she\" , etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words from # http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "# We use frozenset to prevent changes to the set of stop words\n",
    "STOP_WORDS = frozenset([ \n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "    ])\n",
    "\n",
    "# Preprocessing the text data and counting word occurrences\n",
    "words = \" \".join(data['GOALS_DEFINITION'].dropna().str.lower())  # Combine all text entries into one large string\n",
    "words = re.findall(r'\\b\\w+\\b', words)  # Extract words using regex (\\b matches word boundaries)\n",
    "\n",
    "# Remove stop words and count word occurrences\n",
    "filtered_words = [word for word in words if word.lower() not in STOP_WORDS]\n",
    "filtered_word_count = Counter(filtered_words)\n",
    "most_common_filtered_words = filtered_word_count.most_common(20) # Get the 20 most common words\n",
    "\n",
    "# Preparing data for the bar chart\n",
    "filtered_terms, filtered_freqs = zip(*most_common_filtered_words)  # Unzipping the terms and their frequencies\n",
    "\n",
    "# Converting terms and their frequencies to list format for plotting\n",
    "filtered_terms_list = list(filtered_terms)\n",
    "filtered_freqs_list = list(filtered_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting term frequencies across data science definitions\n",
    "\n",
    "Now we can build a simple horizontal bar chart that displays the 20 most common terms across data science definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a horizontal bar chart for the filtered terms\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=filtered_freqs_list, y=filtered_terms_list, color='grey')\n",
    "plt.title('Top 20 most common terms in Data Science definition')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Terms')\n",
    "plt.grid(axis = 'both', alpha = 0.5) # Add grid lines to the plot in both directions\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative plot: Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the filtered terms into one large string\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "# Generating a word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(filtered_text)\n",
    "\n",
    "# Displaying the word cloud\n",
    "plt.figure(figsize = (11, 6))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) bar plots of the frequencies of the background and desired skills \n",
    "\n",
    "Let's now build two horizontal bar charts that show the frequencies of the background and desired skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process and count skills from a column\n",
    "def process_skills_column(skills_column, skills_to_consider):\n",
    "    \"\"\"\n",
    "    Processes and count skills. The function encodes unlisted skills as 'Other'.\n",
    "    \n",
    "    Args:\n",
    "        skills_column (series): A column containing skills data.\n",
    "        skills_to_consider (list): A list of skills to consider.\n",
    "\n",
    "    Returns:\n",
    "        skill_counts (counter): A Counter object with skills and their counts.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    processed_skills_list = []\n",
    "    \n",
    "    for skills_str in skills_column.dropna():\n",
    "        individual_skills = skills_str.split(', ')\n",
    "        # Encoding skills as \"Other\" if they are not in the skills_to_consider list\n",
    "        processed_skills = [skill if skill in skills_to_consider else 'Other' for skill in individual_skills]\n",
    "        processed_skills_list.extend(processed_skills)\n",
    "    \n",
    "    # Counting the occurrences of each skill\n",
    "    skill_counts = Counter(processed_skills_list)\n",
    "    return skill_counts\n",
    "\n",
    "# Defining the skills to consider\n",
    "skills_used = [\n",
    "    \"Relational databases\", \"NoSQL\", \"Information Retrieval\", \"Statistical Analysis\", \n",
    "    \"Visualisation\", \"Machine learning\", \"Data mining\", \"Natural Language Processing\", \n",
    "    \"Programming\", \"Customer Relationship Management\", \"Management\", \"Requirements gathering\", \n",
    "    \"Ethics\", \"Software Engineering\", \"Product-driven thinking\"\n",
    "]\n",
    "\n",
    "# Processing the skills data with encoding unlisted skills as \"Other\"\n",
    "background_skill_counts = process_skills_column(data['BACKGROUND_SKILLS'], skills_used)\n",
    "goals_skill_counts = process_skills_column(data['GOALS_SKILLS'], skills_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skill_distribution(skill_counts, title):\n",
    "    \"\"\"Plots a bar chart for the distribution of skills, ordered by frequency.\n",
    "    \n",
    "    Args:\n",
    "        skill_counts (counter): A counter object with skills and their counts.\n",
    "        title (str): The title for the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sorting the skills by frequency\n",
    "    sorted_skills = {k: v for k, v in sorted(skill_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "    skills, counts = zip(*sorted_skills.items())  # Unzipping the sorted skills and their counts\n",
    "\n",
    "    # Creating the bar plot\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.barplot(x = list(counts), y = list(skills), palette = \"Blues_d\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Number of respondents')\n",
    "    plt.ylabel('Skills')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the skill distributions with sorted bars\n",
    "plot_skill_distribution(background_skill_counts, 'Distribution of background skills')\n",
    "plot_skill_distribution(goals_skill_counts, 'Distribution of desired skills')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
